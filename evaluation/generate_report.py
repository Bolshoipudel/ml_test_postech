"""
–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä markdown –æ—Ç—á–µ—Ç–∞ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ evaluation.

–ß–∏—Ç–∞–µ—Ç JSON —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ evaluation/results/evaluation_results_latest.json
–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–π markdown –æ—Ç—á–µ—Ç —Å:
- Executive Summary
- Router Performance –ø–æ –∫–∞–∂–¥–æ–º—É tool
- DeepEval metrics scores
- Failed test cases —Å reasoning
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é

–ó–∞–ø—É—Å–∫:
    python evaluation/generate_report.py
    python evaluation/generate_report.py --input evaluation/results/custom_results.json
"""

import json
import sys
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime
import argparse


# =============================================================================
# CONFIGURATION
# =============================================================================

DEFAULT_INPUT = "evaluation/results/evaluation_results_latest.json"
DEFAULT_OUTPUT = "evaluation/results/evaluation_report.md"


# =============================================================================
# LOAD RESULTS
# =============================================================================

def load_results(results_file: str) -> Dict[str, Any]:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ JSON —Ñ–∞–π–ª–∞.

    Args:
        results_file: –ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏

    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ evaluation
    """
    try:
        with open(results_file, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"‚ùå Error: Results file not found: {results_file}")
        print(f"   Please run evaluation first: python evaluation/evaluate_system.py")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"‚ùå Error: Invalid JSON in results file: {e}")
        sys.exit(1)


# =============================================================================
# REPORT SECTIONS
# =============================================================================

def generate_header() -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ –æ—Ç—á–µ—Ç–∞."""
    return f"""# LLM Assistant Evaluation Report

**Generated:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

---

"""


def generate_executive_summary(results: Dict[str, Any]) -> str:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Executive Summary —Å–µ–∫—Ü–∏–∏.

    Args:
        results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã evaluation

    Returns:
        Markdown —Å–µ–∫—Ü–∏—è
    """
    metadata = results.get("metadata", {})
    overall = results.get("aggregate_stats", {}).get("overall", {})

    total_tests = metadata.get("total_tests", 0)
    successful = metadata.get("successful_queries", 0)
    failed = metadata.get("failed_queries", 0)
    duration = metadata.get("duration_seconds", 0)
    routing_accuracy = overall.get("routing_accuracy", 0)

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞
    if routing_accuracy >= 0.9:
        status = "üü¢ EXCELLENT"
    elif routing_accuracy >= 0.85:
        status = "üü° GOOD"
    elif routing_accuracy >= 0.7:
        status = "üü† ACCEPTABLE"
    else:
        status = "üî¥ NEEDS IMPROVEMENT"

    return f"""## Executive Summary

**Overall Status:** {status}

| Metric | Value |
|--------|-------|
| **Total Tests** | {total_tests} |
| **Successful Queries** | {successful} ({successful/total_tests*100:.1f}%) |
| **Failed Queries** | {failed} ({failed/total_tests*100:.1f}%) |
| **Overall Routing Accuracy** | {routing_accuracy:.1%} |
| **Evaluation Duration** | {duration:.1f} seconds |
| **API URL** | {metadata.get('api_url', 'N/A')} |
| **Evaluation Date** | {metadata.get('evaluation_date', 'N/A')} |

---

"""


def generate_routing_performance_table(results: Dict[str, Any]) -> str:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü—ã Router Performance.

    Args:
        results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã evaluation

    Returns:
        Markdown —Ç–∞–±–ª–∏—Ü–∞
    """
    by_tool = results.get("aggregate_stats", {}).get("by_tool", {})

    if not by_tool:
        return "## Router Agent Performance\n\nNo routing data available.\n\n---\n\n"

    table = """## Router Agent Performance

| Tool Type | Accuracy | Avg Confidence | Total Tests | Status |
|-----------|----------|----------------|-------------|--------|
"""

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ accuracy (descending)
    sorted_tools = sorted(
        by_tool.items(),
        key=lambda x: x[1].get("routing_accuracy", 0),
        reverse=True
    )

    for tool, stats in sorted_tools:
        accuracy = stats.get("routing_accuracy", 0)
        confidence = stats.get("average_confidence", 0)
        total = stats.get("total", 0)
        correct = stats.get("correct_routing", 0)

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞
        if accuracy >= 0.9:
            status = "‚úÖ PASS"
        elif accuracy >= 0.8:
            status = "‚ö†Ô∏è WARN"
        else:
            status = "‚ùå FAIL"

        table += f"| {tool.upper():9s} | {accuracy:7.1%} | {confidence:14.2f} | {total:11d} | {status:6s} |\n"

    table += "\n**Legend:**\n"
    table += "- ‚úÖ PASS: Accuracy >= 90%\n"
    table += "- ‚ö†Ô∏è WARN: Accuracy >= 80%\n"
    table += "- ‚ùå FAIL: Accuracy < 80%\n\n"
    table += "---\n\n"

    return table


def generate_test_results_summary(results: Dict[str, Any]) -> str:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤–æ–¥–∫–∏ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º —Ç–µ—Å—Ç–æ–≤.

    Args:
        results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã evaluation

    Returns:
        Markdown —Å–µ–∫—Ü–∏—è
    """
    test_results = results.get("test_results", [])

    if not test_results:
        return "## Test Results Summary\n\nNo test results available.\n\n---\n\n"

    # –ü–æ–¥—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    by_category = {}
    failed_tests = []

    for result in test_results:
        category = result.get("category", "unknown")
        expected = result.get("expected_tool", "")
        actual = result.get("actual_tool", "")
        test_id = result.get("test_id", "unknown")

        if category not in by_category:
            by_category[category] = {"total": 0, "correct": 0}

        by_category[category]["total"] += 1

        if expected == actual:
            by_category[category]["correct"] += 1
        else:
            failed_tests.append(result)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    section = """## Test Results by Category

| Category | Total Tests | Correct | Accuracy |
|----------|-------------|---------|----------|
"""

    for category, stats in sorted(by_category.items()):
        total = stats["total"]
        correct = stats["correct"]
        accuracy = correct / total if total > 0 else 0

        section += f"| {category:20s} | {total:11d} | {correct:7d} | {accuracy:8.1%} |\n"

    section += "\n---\n\n"

    return section


def generate_failed_tests_section(results: Dict[str, Any]) -> str:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–µ–∫—Ü–∏–∏ —Å –Ω–µ—É—Å–ø–µ—à–Ω—ã–º–∏ —Ç–µ—Å—Ç–∞–º–∏.

    Args:
        results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã evaluation

    Returns:
        Markdown —Å–µ–∫—Ü–∏—è
    """
    test_results = results.get("test_results", [])

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è failed —Ç–µ—Å—Ç–æ–≤
    failed_tests = [
        r for r in test_results
        if r.get("expected_tool") != r.get("actual_tool")
    ]

    if not failed_tests:
        return """## Failed Tests

‚úÖ **All tests passed routing correctly!**

---

"""

    section = f"""## Failed Tests

Found **{len(failed_tests)}** tests with incorrect routing:

"""

    for i, result in enumerate(failed_tests, 1):
        test_id = result.get("test_id", "unknown")
        query = result.get("query", "")
        expected = result.get("expected_tool", "")
        actual = result.get("actual_tool", "")
        confidence = result.get("confidence", 0)
        reasoning = result.get("reasoning", "")

        section += f"""### {i}. Test ID: `{test_id}`

**Query:** {query}

**Expected Tool:** `{expected.upper()}`
**Actual Tool:** `{actual.upper()}`
**Confidence:** {confidence:.2f}

**Router Reasoning:**
```
{reasoning[:300]}{"..." if len(reasoning) > 300 else ""}
```

---

"""

    return section


def generate_recommendations(results: Dict[str, Any]) -> str:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é.

    Args:
        results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã evaluation

    Returns:
        Markdown —Å–µ–∫—Ü–∏—è
    """
    by_tool = results.get("aggregate_stats", {}).get("by_tool", {})
    overall = results.get("aggregate_stats", {}).get("overall", {})

    routing_accuracy = overall.get("routing_accuracy", 0)

    recommendations = []

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–±—â–µ–π accuracy
    if routing_accuracy < 0.85:
        recommendations.append(
            "**Improve Overall Routing Accuracy**: Current accuracy is below target (85%). "
            "Consider reviewing and improving Router Agent prompts with more few-shot examples."
        )

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º tools
    for tool, stats in by_tool.items():
        accuracy = stats.get("routing_accuracy", 0)
        confidence = stats.get("average_confidence", 0)

        if accuracy < 0.8:
            recommendations.append(
                f"**Improve {tool.upper()} Routing**: Accuracy {accuracy:.1%} is below 80%. "
                f"Add more training examples for {tool} queries in Router prompts."
            )

        if confidence < 0.75:
            recommendations.append(
                f"**Increase {tool.upper()} Confidence**: Average confidence {confidence:.2f} is low. "
                f"Review ambiguous queries and clarify routing logic."
            )

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ failed queries
    metadata = results.get("metadata", {})
    failed_queries = metadata.get("failed_queries", 0)

    if failed_queries > 0:
        recommendations.append(
            f"**Fix Failed Queries**: {failed_queries} queries failed to execute. "
            "Check API stability, timeouts, and error handling."
        )

    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–µ–∫—Ü–∏—è
    if not recommendations:
        return """## Recommendations

‚úÖ **System is performing well!**

All metrics are within acceptable ranges. Continue monitoring performance.

---

"""

    section = "## Recommendations\n\n"

    for i, rec in enumerate(recommendations, 1):
        section += f"{i}. {rec}\n\n"

    section += "---\n\n"

    return section


def generate_metrics_summary(results: Dict[str, Any]) -> str:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤–æ–¥–∫–∏ –ø–æ DeepEval –º–µ—Ç—Ä–∏–∫–∞–º.

    Args:
        results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã evaluation

    Returns:
        Markdown —Å–µ–∫—Ü–∏—è
    """
    deepeval_metrics = results.get("aggregate_stats", {}).get("deepeval_metrics", {})

    if not deepeval_metrics:
        return """## DeepEval Metrics Summary

_No DeepEval metrics data available. Metrics may not have been run during evaluation._

**Metrics Configuration:**
- Answer Relevancy Metric (threshold: 0.7)
- Faithfulness Metric (threshold: 0.7)
- Contextual Relevancy Metric (threshold: 0.7)
- Router Accuracy Metric (threshold: 0.7)

---

"""

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü—ã —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
    section = """## DeepEval Metrics Summary

| Metric Name | Avg Score | Min | Max | Tests | Pass Rate | Status |
|-------------|-----------|-----|-----|-------|-----------|--------|
"""

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –º–µ—Ç—Ä–∏–∫ –ø–æ average_score
    sorted_metrics = sorted(
        deepeval_metrics.items(),
        key=lambda x: x[1].get("average_score", 0),
        reverse=True
    )

    for metric_name, metric_stats in sorted_metrics:
        avg_score = metric_stats.get("average_score", 0)
        min_score = metric_stats.get("min_score", 0)
        max_score = metric_stats.get("max_score", 0)
        count = metric_stats.get("count", 0)
        pass_rate = metric_stats.get("pass_rate", 0)

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞
        if avg_score >= 0.85:
            status = "‚úÖ EXCELLENT"
        elif avg_score >= 0.7:
            status = "üü¢ GOOD"
        elif avg_score >= 0.5:
            status = "üü° ACCEPTABLE"
        else:
            status = "üî¥ POOR"

        section += f"| {metric_name:20s} | {avg_score:9.3f} | {min_score:3.2f} | {max_score:3.2f} | {count:5d} | {pass_rate:9.1%} | {status:13s} |\n"

    section += "\n**Legend:**\n"
    section += "- ‚úÖ EXCELLENT: Average score >= 0.85\n"
    section += "- üü¢ GOOD: Average score >= 0.70\n"
    section += "- üü° ACCEPTABLE: Average score >= 0.50\n"
    section += "- üî¥ POOR: Average score < 0.50\n\n"

    section += "**Metrics Description:**\n"
    section += "- **Answer Relevancy**: Measures how relevant the LLM's answer is to the question\n"
    section += "- **Faithfulness**: Checks if the answer is faithful to the context (no hallucinations)\n"
    section += "- **Contextual Relevancy**: Evaluates quality of retrieved context (for RAG)\n"
    section += "- **Router Accuracy**: Custom metric for routing decision correctness\n\n"

    section += "---\n\n"

    return section


def generate_footer() -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è footer –æ—Ç—á–µ—Ç–∞."""
    return """## Conclusion

This report provides a comprehensive overview of the LLM Assistant evaluation results.
Review the recommendations and failed tests to identify areas for improvement.

For detailed test results, see `evaluation_results_latest.json`.

---

**Generated by:** LLM Assistant Evaluation System
**Framework:** DeepEval + Pytest
**Repository:** https://github.com/PositiveTechnologies/ml_test_postech
"""


# =============================================================================
# MAIN GENERATOR
# =============================================================================

def generate_full_report(results: Dict[str, Any]) -> str:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ markdown –æ—Ç—á–µ—Ç–∞.

    Args:
        results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã evaluation

    Returns:
        –ü–æ–ª–Ω—ã–π markdown –æ—Ç—á–µ—Ç
    """
    report = ""
    report += generate_header()
    report += generate_executive_summary(results)
    report += generate_routing_performance_table(results)
    report += generate_test_results_summary(results)
    report += generate_metrics_summary(results)
    report += generate_failed_tests_section(results)
    report += generate_recommendations(results)
    report += generate_footer()

    return report


def save_report(report: str, output_file: str) -> None:
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –≤ —Ñ–∞–π–ª.

    Args:
        report: Markdown –æ—Ç—á–µ—Ç
        output_file: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    try:
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(report)

        print(f"‚úÖ Report saved to: {output_file}")

    except Exception as e:
        print(f"‚ùå Error saving report: {e}")
        sys.exit(1)


# =============================================================================
# CLI
# =============================================================================

def parse_arguments() -> argparse.Namespace:
    """–ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏."""
    parser = argparse.ArgumentParser(
        description="Generate markdown evaluation report from JSON results"
    )

    parser.add_argument(
        "--input",
        type=str,
        default=DEFAULT_INPUT,
        help=f"Input JSON file with evaluation results (default: {DEFAULT_INPUT})"
    )

    parser.add_argument(
        "--output",
        type=str,
        default=DEFAULT_OUTPUT,
        help=f"Output markdown file (default: {DEFAULT_OUTPUT})"
    )

    parser.add_argument(
        "--print",
        action="store_true",
        help="Print report to stdout in addition to saving"
    )

    return parser.parse_args()


def main():
    """Main entry point."""
    args = parse_arguments()

    print("="*80)
    print("LLM Assistant - Evaluation Report Generator")
    print("="*80)
    print(f"\nInput:  {args.input}")
    print(f"Output: {args.output}\n")

    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("Loading results...")
    results = load_results(args.input)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
    print("Generating report...")
    report = generate_full_report(results)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
    save_report(report, args.output)

    # –í—ã–≤–æ–¥ –≤ stdout (–µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–æ)
    if args.print:
        print("\n" + "="*80)
        print("REPORT PREVIEW")
        print("="*80 + "\n")
        print(report)

    print("\n‚úÖ Report generation completed successfully!")
    print(f"\nView report: cat {args.output}")


if __name__ == "__main__":
    main()
